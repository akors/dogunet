{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets\n",
    "import torch.utils.data\n",
    "import monai.metrics\n",
    "\n",
    "import transforms\n",
    "import datasets\n",
    "\n",
    "from brain_segmentation_pytorch.unet import UNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inf = transforms.make_transforms(transforms.PASCAL_VOC_2012_MEAN, transforms.PASCAL_VOC_2012_STD)\n",
    "tr_img_inv = transforms.inv_normalize(transforms.PASCAL_VOC_2012_MEAN, transforms.PASCAL_VOC_2012_STD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = torchvision.datasets.wrap_dataset_for_transforms_v2(torchvision.datasets.VOCSegmentation(\n",
    "    root=dataroot,\n",
    "    year=\"2012\",\n",
    "    image_set=\"train\",\n",
    "    download=False,\n",
    "    transforms=tr_inf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_val = torchvision.datasets.wrap_dataset_for_transforms_v2(torchvision.datasets.VOCSegmentation(\n",
    "    root=dataroot,\n",
    "    year=\"2012\",\n",
    "    image_set=\"val\",\n",
    "    download=False,\n",
    "    transforms=tr_inf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointfile = \"../checkpoints/test.chpt.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from checkpoint file\n",
    "checkpoint = torch.load(checkpointfile)\n",
    "unet_features = checkpoint['model_state_dict']['encoder1.enc1conv1.weight'].size(0)\n",
    "model = UNet(\n",
    "    in_channels=3,\n",
    "    out_channels=datasets.CLASS_MAX+1,\n",
    "    init_features=unet_features,\n",
    ")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topclass_dict(mask: torch.Tensor, k: int=4):\n",
    "    pred_classes, counts = mask.unique(return_counts=True)\n",
    "    topcounts, topcounts_idx = torch.topk(counts, min(k, len(counts)))\n",
    "    #topclasses = pred_classes[topcounts_idx]\n",
    "\n",
    "    top_k_classpixels = {\n",
    "        datasets.CLASSNAMES[pred_classes[i].item()] : counts[i].item()\n",
    "        for i in topcounts_idx\n",
    "    }\n",
    "\n",
    "    return top_k_classpixels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "This section calculates some metrics through different ways:\n",
    " - Manually comparix pixel values\n",
    " - Metrics classes from MonAI\n",
    " - Metrics based on ConfusionMatrixMetrix from MONAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.stack([ds_train[i][0] for i in range(3)])\n",
    "mask = torch.stack([ds_train[i][1] for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(img.to(device))\n",
    "    pred = pred.to(device='cpu')\n",
    "    pred_amax = torch.argmax(pred, dim=1, keepdim=True).to(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 256, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_amax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'background': 127721, 'chair': 38425, 'tv/monitor': 13871, 'dog': 12967}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topclass_dict(pred_amax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required format for MONAI metrics:\n",
    "# float, but 1.0 at whatever we define as predicting a class, 0.0 elsewhere\n",
    "pred_binarized = torch.zeros_like(pred).scatter_(1, pred_amax, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12 is dog\n",
    "pred_amax[2,:,128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9 is chair\n",
    "pred_amax[2,:,10,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 21, 256, 256])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_binarized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class slice through center pixel, should be dog\n",
    "pred_binarized[2,:,128,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class slice through top center pixel, should be chair\n",
    "pred_binarized[2,:,9,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this better be 1\n",
    "pred_binarized.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_onehot = torch.zeros_like(pred).scatter_(1, mask.to(dtype=torch.long), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 21, 256, 256])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_onehot[2,:,9,128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61316,  3884,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,   336,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [51571,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         13965],\n",
       "        [15904,     0,     0,     0,     0,     0,     0,     0,     0, 36763,\n",
       "             0,     0, 12869,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of pixels in the mask per class per batch\n",
    "pixelcount_mask = mask_onehot.count_nonzero(dim=(-2,-1))\n",
    "pixelcount_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[61918,  3381,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            37,     0,     0,     0,     0,   200,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [51665,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         13871],\n",
       "        [14138,     0,     0,     0,     0,     0,     0,     0,     1, 38425,\n",
       "             0,     0, 12967,     0,     0,     5,     0,     0,     0,     0,\n",
       "             0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of pixels in the prediction per class per batch\n",
    "pixelcount_pred = pred_binarized.count_nonzero(dim=(-2,-1))\n",
    "pixelcount_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60792,  2863,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,   178,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [51276,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         13576],\n",
       "        [13297,     0,     0,     0,     0,     0,     0,     0,     0, 36163,\n",
       "             0,     0, 12518,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of pixels BOTH in the mask AND prediction per class per batch\n",
    "pixelcount_intersection = torch.logical_and(mask_onehot, pred_binarized).count_nonzero(dim=(-2,-1))\n",
    "pixelcount_intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62442,  4402,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            37,     0,     0,     0,     0,   358,     0,     0,     0,     0,\n",
       "             0],\n",
       "        [51960,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "         14260],\n",
       "        [16745,     0,     0,     0,     0,     0,     0,     0,     1, 39025,\n",
       "             0,     0, 13318,     0,     0,     5,     0,     0,     0,     0,\n",
       "             0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of pixels EITHER in the mask OR prediction per class per batch\n",
    "pixelcount_union = torch.logical_or(mask_onehot, pred_binarized).count_nonzero(dim=(-2,-1))\n",
    "pixelcount_union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Accuracy\n",
    "\n",
    "or Old Pixel-Wise Accuracy\n",
    "\n",
    "Simply the sum of the correctly predicted pixels in this batch divided by the total number of pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 256, 256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 256, 256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_amax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9698)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (mask == pred_amax)\n",
    "acc = (acc.sum()/acc.numel())\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9698)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (mask == pred_amax).count_nonzero() / mask.numel()\n",
    "acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Score / F1-Score\n",
    "\n",
    "https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient\n",
    "\n",
    "Sets:\n",
    "\n",
    "`DSC = 2*|A^B| / (|A| + |B|)`\n",
    "\n",
    "\n",
    "Bool only:\n",
    "\n",
    "`DSC = 2*FP / (2*TP + FP + FN)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample-Wise Dice score\n",
    "\n",
    "Only pixels in one prediction will be compared to pixels in the corresponding target mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9866, 0.7882,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan, 0.0000,    nan,    nan,    nan,    nan, 0.6642,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9934,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9754],\n",
       "        [0.8852,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "         0.9619,    nan,    nan, 0.9690,    nan,    nan, 0.0000,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_samplewise = 2 * pixelcount_intersection / (pixelcount_pred + pixelcount_mask)\n",
    "dice_samplewise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean Dice\n",
    "\n",
    "All pixels of the dataset within one class will be added up and a per-class dice score calculated from that.\n",
    "The overall mean dice is the per-class dice score averaged over the classes.\n",
    "\n",
    "This has the effect that underrepresented classes contribute equally to the overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9775, 0.7882,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.9619, 0.0000,    nan, 0.9690,    nan,    nan, 0.6580,    nan,    nan,\n",
       "           nan,    nan, 0.9754])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate pixels over batches\n",
    "\n",
    "dice_batchagg = 2.0 * pixelcount_intersection.sum(dim=0) / (pixelcount_pred + pixelcount_mask).sum(dim=0)\n",
    "dice_batchagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6663)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate avarage over classes. This is our meanDice\n",
    "dice_batchagg.nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore prediction of classes that were not in the mask, corresponds to ignore_empty=True\n",
    "# this is for comparing with monai.metrics.DiceMetrics\n",
    "dn = (pixelcount_pred + pixelcount_mask)\n",
    "dn_nonempty = torch.where(pixelcount_mask > 0, dn, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9775, 0.7882,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "        0.9619,    nan,    nan, 0.9690,    nan,    nan, 0.6642,    nan,    nan,\n",
       "           nan,    nan, 0.9754])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate pixels over batches\n",
    "\n",
    "dice_batchagg_nonempty = 2.0 * pixelcount_intersection.sum(dim=0) / dn_nonempty.sum(dim=0)\n",
    "dice_batchagg_nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8894)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then average over classes\n",
    "mean_dice_nonempty = dice_batchagg_nonempty.nanmean()\n",
    "mean_dice_nonempty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### monai.metrics.DiceMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = monai.metrics.DiceMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9866, 0.7882,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan, 0.6642,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9934,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9754],\n",
       "        [0.8852,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "         0.9619,    nan,    nan, 0.9690,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = metric(pred_binarized, mask_onehot)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9551, 0.7882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.9619, 0.0000, 0.0000, 0.9690, 0.0000, 0.0000, 0.6642, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.9754])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.aggregate(reduction=\"mean_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9120])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_samplewise_nonempty = torch.where(pixelcount_mask != 0, dice_samplewise, torch.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9120)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_samplewise_nonempty.nanmean(dim=1).nanmean(dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU (Intersection over Union / Jaccard)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "Sets:\n",
    "\n",
    "`DSC = |A^B| / (|A| v |B|)`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample-Wise IoU\n",
    "\n",
    "Only pixels in one prediction will be compared to pixels in the corresponding target mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9736, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan, 0.0000,    nan,    nan,    nan,    nan, 0.4972,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9868,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9520],\n",
       "        [0.7941,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "         0.9267,    nan,    nan, 0.9399,    nan,    nan, 0.0000,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IoU per one sample\n",
    "iou_samplewise = pixelcount_intersection / pixelcount_union\n",
    "iou_samplewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9736, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan, 0.0000,    nan,    nan,    nan,    nan, 0.4972,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9868,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9520],\n",
       "        [0.7941,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "         0.9267,    nan,    nan, 0.9399,    nan,    nan, 0.0000,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IoU per one sample, calculated from values available in confusion matrix should be same as above\n",
    "iou_samplewise = pixelcount_intersection / (pixelcount_pred + pixelcount_mask - pixelcount_intersection)\n",
    "iou_samplewise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mIoU (mean IoU / mean Jaccard Index)\n",
    "\n",
    "All pixels of the dataset within one class will be added up and a per-class IoU score calculated from that.\n",
    "The overall mean IoU is the per-class IoU score averaged over the classes.\n",
    "\n",
    "This has the effect that underrepresented classes contribute equally to the overall score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9559, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.9267, 0.0000,    nan, 0.9399,    nan,    nan, 0.4904,    nan,    nan,\n",
       "           nan,    nan, 0.9520])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate pixels over batches\n",
    "\n",
    "my_iou_batchagg = pixelcount_intersection.sum(dim=0) / (pixelcount_pred + pixelcount_mask - pixelcount_intersection).sum(dim=0)\n",
    "my_iou_batchagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn nans into ones, this corresponds to ignore_empty=False\n",
    "my_iou_batchagg_withempty = my_iou_batchagg.nan_to_num(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8531)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then average over classes\n",
    "my_mean_iou_withempty = my_iou_batchagg_withempty.mean()\n",
    "my_mean_iou_withempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore prediction of classes that were not in the mask, corresponds to ignore_empty=True\n",
    "# TODO: Think hard about whether this is actually a good idea\n",
    "pixelcount_union_nonempty = torch.where(pixelcount_mask > 0, pixelcount_union, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9559, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "        0.9267,    nan,    nan, 0.9399,    nan,    nan, 0.4972,    nan,    nan,\n",
       "           nan,    nan, 0.9520])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate pixels over batches\n",
    "\n",
    "iou_batchagg_nonempty = pixelcount_intersection.sum(dim=0) / pixelcount_union_nonempty.sum(dim=0)\n",
    "iou_batchagg_nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8204)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then average over classes\n",
    "mean_iou_nonempty = iou_batchagg_nonempty.nanmean()\n",
    "mean_iou_nonempty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### monai.metrics.MeanIoU()\n",
    "\n",
    "This calculates a \"mean\" IoU, but the wrong one: it first calculates the sample IoU, then averages over channels then\n",
    "over batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = monai.metrics.MeanIoU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9736, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan, 0.4972,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9868,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9520],\n",
       "        [0.7941,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "         0.9267,    nan,    nan, 0.9399,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = metric(y_pred=pred_binarized, y=mask_onehot)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8545])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.aggregate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9736, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan, 0.4972,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9868,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9520],\n",
       "        [0.7941,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "         0.9267,    nan,    nan, 0.9399,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nan out classes that were not in the target mask at all\n",
    "# This is equivalent to MONAI metrics ignore_empty=True\n",
    "iou_samplewise_nonempty = torch.where(pixelcount_mask != 0, iou_samplewise, torch.nan)\n",
    "iou_samplewise_nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8545)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MeanIoU from MonAI is equivalent to averaging sample-wise IoU scores\n",
    "iou_samplewise_nonempty.nanmean(dim=1).nanmean(dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics based on Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm_metrics_types = [\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"accuracy\",\n",
    "    \"f1 score\", # aka Dice Score\n",
    "    \"threat score\" # aka Intersection over Union aka Jaccard Index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm_metric = monai.metrics.ConfusionMatrixMetric(metric_name=confm_metrics_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm = confm_metric(y_pred=pred_binarized, y=mask_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 21, 4])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[60792.,  2863.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,   178.,     0.,     0.,\n",
       "             0.,     0.,     0.],\n",
       "        [51276.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0., 13576.],\n",
       "        [13297.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "         36163.,     0.,     0., 12518.,     0.,     0.,     0.,     0.,     0.,\n",
       "             0.,     0.,     0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true positives\n",
    "tp = confm[:,:,0]\n",
    "tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1260e+03, 5.1800e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7000e+01, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.2000e+01, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "        [3.8900e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 2.9500e+02],\n",
       "        [8.4100e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 1.0000e+00, 2.2620e+03, 0.0000e+00, 0.0000e+00,\n",
       "         4.4900e+02, 0.0000e+00, 0.0000e+00, 5.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00, 0.0000e+00]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# false positives\n",
    "fp = confm[:,:,1]\n",
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3094., 61134., 65536., 65536., 65536., 65536., 65536., 65536., 65536.,\n",
       "         65536., 65499., 65536., 65536., 65536., 65536., 65178., 65536., 65536.,\n",
       "         65536., 65536., 65536.],\n",
       "        [13576., 65536., 65536., 65536., 65536., 65536., 65536., 65536., 65536.,\n",
       "         65536., 65536., 65536., 65536., 65536., 65536., 65536., 65536., 65536.,\n",
       "         65536., 65536., 51276.],\n",
       "        [48791., 65536., 65536., 65536., 65536., 65536., 65536., 65536., 65535.,\n",
       "         26511., 65536., 65536., 52218., 65536., 65536., 65531., 65536., 65536.,\n",
       "         65536., 65536., 65536.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true negatives\n",
    "tn = confm[:,:,2]\n",
    "tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 524., 1021.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "            0.,    0.,    0.,    0.,    0.,  158.,    0.,    0.,    0.,    0.,\n",
       "            0.],\n",
       "        [ 295.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "            0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          389.],\n",
       "        [2607.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,  600.,\n",
       "            0.,    0.,  351.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "            0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# false negatives\n",
    "fn = confm[:,:,3]\n",
    "fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9818, 0.8468,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan, 0.0000,    nan,    nan,    nan,    nan, 0.8900,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9925,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9787],\n",
       "        [0.9405,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "         0.9411,    nan,    nan, 0.9654,    nan,    nan, 0.0000,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# precision (sample-wise)\n",
    "tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9816, 0.8468,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.9411, 0.0000,    nan, 0.9654,    nan,    nan, 0.8683,    nan,    nan,\n",
       "           nan,    nan, 0.9787])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class precision of batch aggregate\n",
    "precision_batchagg = tp.sum(dim=0) / (tp + fp).sum(dim=0)\n",
    "precision_batchagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9816, 0.8468,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.9411, 0.0000,    nan, 0.9654,    nan,    nan, 0.8683,    nan,    nan,\n",
       "           nan,    nan, 0.9787])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class precision of batch aggregate, calculated through the confusion matrix metric\n",
    "# this is what we want\n",
    "confm_metric.aggregate(reduction=\"sum_batch\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6977)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall precision (batch aggregate precision averaged over non-nan classes)\n",
    "# this is what we want\n",
    "precision_batchagg.nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9698])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduction of confusion matrix values prior to metric calculation\n",
    "# This is NOT what we want.\n",
    "confm_metric.aggregate(reduction=\"mean\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9698)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.sum() / (tp.sum() + fp.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9915, 0.7371,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan, 0.5298,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9943,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9721],\n",
       "        [0.8361,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "         0.9837,    nan,    nan, 0.9727,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall (sample-wise)\n",
    "tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9734, 0.7371,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "        0.9837,    nan,    nan, 0.9727,    nan,    nan, 0.5298,    nan,    nan,\n",
       "           nan,    nan, 0.9721])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class recall of batch aggregate.\n",
    "# This is what we want\n",
    "tp.sum(dim=0) / (tp + fn).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9734, 0.7371,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "        0.9837,    nan,    nan, 0.9727,    nan,    nan, 0.5298,    nan,    nan,\n",
       "           nan,    nan, 0.9721])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class recall, calculated through the confusion matrix metric\n",
    "# this is what we want\n",
    "confm_metric.aggregate(reduction=\"sum_batch\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8615)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall recall\n",
    "torch.nanmean(tp.sum(dim=0) / (tp + fn).sum(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9698])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confm_metric.aggregate(reduction=\"mean\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9698)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.sum() / (tp.sum() + fn.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Accuracy from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9748, 0.9765, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 0.9994, 1.0000, 1.0000, 1.0000, 1.0000, 0.9973, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.9896, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.9896],\n",
       "        [0.9474, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         0.9563, 1.0000, 1.0000, 0.9878, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample-wise accuracy from confusion matrix\n",
    "(tp + tn) / (tp + fp + tn + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9748, 0.9765, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 0.9994, 1.0000, 1.0000, 1.0000, 1.0000, 0.9973, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000],\n",
       "        [0.9896, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.9896],\n",
       "        [0.9474, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         0.9563, 1.0000, 1.0000, 0.9878, 1.0000, 1.0000, 0.9999, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class batch aggregate accuracy\n",
    "confm_metric.aggregate(reduction=\"none\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190663.)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ConfusionMatrixMetric aggregates hits per-class. We can recover the overall hits by summing over only the\n",
    "# true positives and NOT the true negatives, because the true negatives from each class will be contained within\n",
    "# the true positives of the matching class\n",
    "tp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190663)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask == pred_amax).count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5945.)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incorrectly identified pixels will count towards the \"false positives\"\n",
    "fp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5945)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask != pred_amax).count_nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(196608.)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of pixels from confusion matrix\n",
    "(tp+fp).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196608"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9698)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overall accuracy calculated from per-class TP/FP\n",
    "tp.sum() / (tp+fp).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean Accuracy from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9706, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.9854, 0.9998, 1.0000, 0.9959, 1.0000, 1.0000, 0.9991, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 0.9965])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class batch aggregate accuracy, calculated through the confusion matrix metric\n",
    "(tp + tn).sum(dim=0) / (tp + tn + fp + fn).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9971)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean accuracy aka per-class averaged accuracy\n",
    "(tp + tn).sum() / (tp + tn + fp + fn).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9706, 0.9922, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        0.9854, 0.9998, 1.0000, 0.9959, 1.0000, 1.0000, 0.9991, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 0.9965])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class batch aggregate accuracy\n",
    "confm_metric.aggregate(reduction=\"sum_batch\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9971])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean accuracy aka per-class averaged accuracy\n",
    "confm_metric.aggregate(reduction=\"sum\")[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean Dice from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9866, 0.7882,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan, 0.0000,    nan,    nan,    nan,    nan, 0.6642,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9934,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9754],\n",
       "        [0.8852,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "         0.9619,    nan,    nan, 0.9690,    nan,    nan, 0.0000,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample-wise dice from confusion matrix\n",
    "2.0*tp / (2.0 * tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9866, 0.7882,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan, 0.0000,    nan,    nan,    nan,    nan, 0.6642,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9934,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9754],\n",
       "        [0.8852,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "         0.9619,    nan,    nan, 0.9690,    nan,    nan, 0.0000,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_samplewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9775, 0.7882,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.9619, 0.0000,    nan, 0.9690,    nan,    nan, 0.6580,    nan,    nan,\n",
       "           nan,    nan, 0.9754])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class batch aggregate Dice, calculated through the confusion matrix metric\n",
    "# this is what we want\n",
    "2.0*tp.sum(dim=0) / (2.0 * tp + fn + fp).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9775, 0.7882,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.9619, 0.0000,    nan, 0.9690,    nan,    nan, 0.6580,    nan,    nan,\n",
       "           nan,    nan, 0.9754])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class batch aggregate IoU\n",
    "# this is what we want\n",
    "confm_metric.aggregate(reduction=\"sum_batch\")[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean IoU from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9736, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan, 0.0000,    nan,    nan,    nan,    nan, 0.4972,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9868,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9520],\n",
       "        [0.7941,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "         0.9267,    nan,    nan, 0.9399,    nan,    nan, 0.0000,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample-wise IoU from confusion matrix\n",
    "tp / (tp + fn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9736, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan, 0.0000,    nan,    nan,    nan,    nan, 0.4972,    nan,    nan,\n",
       "            nan,    nan,    nan],\n",
       "        [0.9868,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "            nan,    nan, 0.9520],\n",
       "        [0.7941,    nan,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "         0.9267,    nan,    nan, 0.9399,    nan,    nan, 0.0000,    nan,    nan,\n",
       "            nan,    nan,    nan]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare to manually calculated sample-wise IoU for comparison\n",
    "iou_samplewise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9559, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.9267, 0.0000,    nan, 0.9399,    nan,    nan, 0.4904,    nan,    nan,\n",
       "           nan,    nan, 0.9520])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class batch aggregate IoU, calculated through the confusion matrix metric\n",
    "# this is what we want\n",
    "tp.sum(dim=0) / (tp + fn + fp).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9559, 0.6504,    nan,    nan,    nan,    nan,    nan,    nan, 0.0000,\n",
       "        0.9267, 0.0000,    nan, 0.9399,    nan,    nan, 0.4904,    nan,    nan,\n",
       "           nan,    nan, 0.9520])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# per-class batch aggregate IoU\n",
    "# this is what we want\n",
    "confm_metric.aggregate(reduction=\"sum_batch\")[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9413])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduction of confusion matrix values prior to metric calculation\n",
    "# This is NOT what we want.\n",
    "confm_metric.aggregate(reduction=\"sum\")[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9413)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduction of confusion matrix values prior to metric calculation\n",
    "# This is NOT what we want.\n",
    "confm_iou_mean = tp.sum() / (tp.sum() + fn.sum() + fp.sum())\n",
    "confm_iou_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89104300547b5a491684fb48aa5fbbf869df96c0a93b8b24f3232becaed215cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
